{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panzershracker/Neural-networks-introducing/blob/master/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2lmUIg9Q1Hb",
        "colab_type": "text"
      },
      "source": [
        "# Введение в искусственные нейронные сети\n",
        "# Урок 5. Рекуррентные нейронные сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KWBZ915Q1Hd",
        "colab_type": "text"
      },
      "source": [
        "## Содержание методического пособия:\n",
        "\n",
        "\n",
        "<ol>\n",
        "<li>Что такое Рекурретные нейронные сети</li>\n",
        "<li>Архитектура Рекуррентных нейронных сетей</li>\n",
        "<li>Пример на Keras рекуррентной нейронной сети</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iohWiZXXQ1He",
        "colab_type": "text"
      },
      "source": [
        "## Что такое Рекуррентные нейронные сети\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAWN0sgHQ1Hf",
        "colab_type": "text"
      },
      "source": [
        "Нейронные сети которые мы разбирали ранее относяться к классу feed forward нейронных сетей или сетей прямого распространения. Выходной сигнал слоя в этих нейронных сетях передавался напрямую в следующий слой. Однако есть задачи, в которых нам нужно обучать нейронную сеть не на единичных экземплярах наподобие изображений, а на наборах последовательностей, например последовательностей слов. \n",
        "\n",
        "В рекуррентной нейронной сети выходной сигнал внутренних слоев циркулирует в этих слоях некоторое время. При обучении такой нейронной сети прежние выходные сигналы используются как дополнительные input'ы. Можно сказать, что эти дополнительные input'ы конкатенируются с \"нормальными\" input'ами предыдущего слоя.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_5v-ogHQ1Hg",
        "colab_type": "text"
      },
      "source": [
        "![1.png](attachment:1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csjtCO00Q1Hg",
        "colab_type": "text"
      },
      "source": [
        "Источник: https://medium.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCrtVNQaQ1Hi",
        "colab_type": "text"
      },
      "source": [
        "Реккурентные нейронные сети используются например для того, чтобы научить компьютерные системы \"понимать\" человеческих язык, для генерации текста. Также нейронные сети с подобной архитектурой могут использоваться для любых задач где осуществляется работа с некоторыми последовательностями значений, например с биржевыми котировками. Разновидности реккуретных нейронных используются также для постороения ИИ, подобных тем, что обыграли человека в компьютерную игру Dota 2. В отличие от сверточных нейронных сетей реккурентные нейронные сети как правило содержат небольшое количество слоев и например рекуррентая нейронная сеть в несколько десятков слоев будет считаться большой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQuQajFYQ1Hi",
        "colab_type": "text"
      },
      "source": [
        "## Архитектура Рекуррентных нейронных сетей\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yysVh_v0Q1Hj",
        "colab_type": "text"
      },
      "source": [
        "Несмотря на то, что RNN могут хорошо справляться со своими задачами они не могут работать с длинными последовательностями. Эффективно они могут работать только с последовательностями состоящими из 3-4 элементов. Для, к примеру, анализа текста отзывов на предмет того положительный это отзыв или нет этого будет недостаточно. Здесь может понадобиться анализ нескольких десятков слов, чтобы сделать корректный вывод. Давайте обсудим почему обычной RNN не удается анализировать длинные последовательности. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCSmA2jQ1Hk",
        "colab_type": "text"
      },
      "source": [
        "#### Vanishing gradient problem\n",
        "\n",
        "Из материалов по сверточным нейронным сетям нам известна проблема исчезающего градиента. В случае с большим количеством слоев значение градиента при последовательном обновлении большого количества слоев становиться все меньше и может стать настолько маленьким, что не сможет в принципе существенно изменить поведение нейронов. В реккурентных нейронных сетях из-за сигнала циркулирующего внуртри слоев это проблема становиться еще острее. Причем градиент может стать не только очень маленьким, но и очень большим.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1UPkOmbQ1Hl",
        "colab_type": "text"
      },
      "source": [
        "![2.png](attachment:2.png)![2.png](attachment:2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tvmWmLNQ1Hm",
        "colab_type": "text"
      },
      "source": [
        "Источник: https://medium.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ_3F6ldQ1Hn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMek2JYjQ1Ho",
        "colab_type": "text"
      },
      "source": [
        "### Long Short Term Memory(LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pYtXCHmQ1Hp",
        "colab_type": "text"
      },
      "source": [
        "Решить проблему исчезающего градиента призвана разновидность RNN под названием LSTM.\n",
        "\n",
        "Long short-term memory (LSTM) юниты - это блоки из которых состоят слои одной из разновидностей рекуррентной нейронной сети(RNN). RNN состоящая из LSTM юнитов иногда называется просто LSTM. Обычно LSTM юнит представляет из себя ячейку состоящую из input gate, output gate и forget gate. Эти ячейки отвественны за запоминания значений на определенные промежутки времени.\n",
        "\n",
        "Каждый из этих элементов можно представить как типичный искусственный нейрон в многослойной неройнной сети, они вычисляют активацию(используя функцию активации) как взвешенную сумму. Их работа сводиться к регуляции потока значений через блок LSTM, поэтому они и называются ворота или затворы(gate). Понятие долгой памяти в названии возникло из-за того что они могут запоминать информацию на более длинный период времени чем обычная RNN. LSTM хорошо подходит для классификации процессов и предсказания временных последовательностей неизвестного размера и неизвестных промежутков между важными событиями. С технической точки зрения это достигается за счет ликвидации проблем связанных с exploding и vanishing gradient'ами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9IOOT-KQ1Hq",
        "colab_type": "text"
      },
      "source": [
        "![3.png](attachment:3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4-vruyLQ1Hr",
        "colab_type": "text"
      },
      "source": [
        "Источник: https://medium.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gnfxEkCQ1Hs",
        "colab_type": "text"
      },
      "source": [
        "### Компоненты LSTM.  \n",
        "Ниже спискок приведен список компонентов из которых состоит ячейка LSTM:\n",
        "\n",
        "\n",
        "<ol>\n",
        "<li>Forget Gate \"f\" (нейронная сеть с сигмоидой)</li>\n",
        "<li>Candidate layer \"С\" (нейронная сеть c Tanh)</li>\n",
        "<li>Input Gate \"I\" (нейронная сеть с сигмоидой)</li>\n",
        "<li>Output Gate \"O\"(нейронная сеть с сигмоидой)</li>\n",
        "<li>Скрытоое состояние \"H\" (вектор)</li>\n",
        "<li>Состояние памяти \"C\" (вектор)</li>\n",
        "<li>Входы в LSTM ячейку на любом шаге Xt (текущий input) , Ht-1 (предыдущее скрытое сотояние ) и Ct-1 (предыдущее состояние памяти)</li>\n",
        "<li>Выходы LSTM ячейки это Ht (текущее скрытое состояние ) и Ct (текущее состояние памяти)</li>\n",
        "</ol>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKUsdHl1Q1Ht",
        "colab_type": "text"
      },
      "source": [
        "###### Работа затворов в LSTM\n",
        "\n",
        "\n",
        "Во-первых LSTM ячейка берет предыдущее состояние памяти Ct-1 и умножает на значение в forget gate(f), чтобы определить, присутствует ли состояние памяти Ct. Если forget gate значение равно 0 то предыдущее состояние память полностью забывается, если же f forget gate значение равно 1 то предыдующее значение состояния памяти полностью проходит через ячейку(помните, что f gate дает значение между 0 и 1).\n",
        "\n",
        "Ct = Ct-1 * ft\n",
        "\n",
        "Вычисляем новое состояние памяти:\n",
        "\n",
        "Ct = Ct + (It * C`t)\n",
        "\n",
        "Теперь, вычисляем выходное значение:\n",
        "\n",
        "Ht = tanh(Ct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdI9ooM2Q1Hu",
        "colab_type": "text"
      },
      "source": [
        "### GRU\n",
        "\n",
        "Теперь, когда мы знаем как работает LSTM, давайте бегло взглянем на то, как работате GRU. GRU это более новое поколение рекуррентных нейронных сетейи и оно во многом похоже на LSTM. Но есть определенная разница. В GRU не используется состояние яченийки и используется скрытое состояние для передачи информации. В GRU также есть два затвора - reset gate и update gate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HoKxS7fQ1Hv",
        "colab_type": "text"
      },
      "source": [
        "![4.png](attachment:4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jk5-L33Q1Hw",
        "colab_type": "text"
      },
      "source": [
        "Источник: https://medium.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxhkvCWbQ1Hw",
        "colab_type": "text"
      },
      "source": [
        "Update Gate обновляет затворы действуя подобно forget и input gate, которые используются в LSTM. Он решает какая информация будет отброшена, а какая новая информация будет добавлена. Reset Gate это другой затвор использующийся для принятия решения как много прошлой информации будет забыто. В этих особенностях и заключается архитектура GRU. GRU имеет меньше тензорных операций и соответсвенно тренеруется быстре чем LSTM. Однако нельзя сказать точно какая архитектура лучше. Исследователи и инженеры пытаются определить, что в каждом конкретном случае подойдет больше. Если говорить упрощенно то GRU может подойти тогда когда важнее скорость чем точность, а LSTM тогда когда важнее точность чем скорость. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMDJLr_YQ1Hx",
        "colab_type": "text"
      },
      "source": [
        "## Практика"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiU2keNqQ1Hy",
        "colab_type": "text"
      },
      "source": [
        "Давайте попробуем сделать простую реккурентную нейронную сеть, которая будет учиться складывать числа. Для этих целей мы не будем пользоваться фреймворками для Deep Learning, чтобы посмотреть как она работает внутри.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGfO6XI5Q1Hz",
        "colab_type": "code",
        "outputId": "266d2ae8-7030-4eb9-c276-88df82790ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "# впервую очередь подключим numpy и библиотеку copy, которая понадобиться, чтобы сделать deepcopy ряда элементов\n",
        "\n",
        "import copy, numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# вычислим сигмоиду\n",
        "def sigmoid(x):\n",
        "    output = 1/(1+np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# конвертируем значение функции сигмоиды в ее производную. \n",
        "def sigmoid_output_to_derivative(output):\n",
        "    return output*(1-output)\n",
        "\n",
        "# генерация тренировочного датасета\n",
        "int2binary = {}\n",
        "binary_dim = 8\n",
        "\n",
        "largest_number = pow(2,binary_dim)\n",
        "binary = np.unpackbits(\n",
        "    np.array([list(range(largest_number))],dtype=np.uint8).T,axis=1)\n",
        "for i in range(largest_number):\n",
        "    int2binary[i] = binary[i]\n",
        "\n",
        "# входные переменные\n",
        "alpha = 0.1\n",
        "input_dim = 2\n",
        "hidden_dim = 16\n",
        "output_dim = 1\n",
        "\n",
        "\n",
        "# инициализация весов нейронной сети\n",
        "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
        "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
        "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
        "\n",
        "synapse_0_update = np.zeros_like(synapse_0)\n",
        "synapse_1_update = np.zeros_like(synapse_1)\n",
        "synapse_h_update = np.zeros_like(synapse_h)\n",
        "\n",
        "# тренировочная логика\n",
        "for j in range(10000):\n",
        "    \n",
        "    # генерация простой проблемы сложения (a + b = c)\n",
        "    a_int = np.random.randint(largest_number/2) # int version\n",
        "    a = int2binary[a_int] # бинарное кодирование\n",
        "\n",
        "    b_int = np.random.randint(largest_number/2) # int version\n",
        "    b = int2binary[b_int] # бинарное кодирование\n",
        "\n",
        "    # правильный ответ\n",
        "    c_int = a_int + b_int\n",
        "    c = int2binary[c_int]\n",
        "    \n",
        "    # место где мы располагаем наши лучше результаты (бинарно закодированные)\n",
        "    d = np.zeros_like(c)\n",
        "\n",
        "    overallError = 0\n",
        "    \n",
        "    layer_2_deltas = list()\n",
        "    layer_1_values = list()\n",
        "    layer_1_values.append(np.zeros(hidden_dim))\n",
        "    \n",
        "    # движение вдоль позиций бинарной кодировки\n",
        "    for position in range(binary_dim):\n",
        "        \n",
        "        # генерация input и output\n",
        "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
        "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
        "\n",
        "        # внутренний слой (input ~+ предыдущий внутренний)\n",
        "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
        "\n",
        "        # output layer (новое бинарное представление)\n",
        "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
        "\n",
        "        # проверка упустили ли мы что-то и если да, то как много \n",
        "        layer_2_error = y - layer_2\n",
        "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
        "        overallError += np.abs(layer_2_error[0])\n",
        "    \n",
        "        # декодируем оценку чтобы мы могли ее вывести на экран\n",
        "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
        "        \n",
        "        # сохраняем внутренний слой, чтобы мы могли его использовать в след. timestep\n",
        "        layer_1_values.append(copy.deepcopy(layer_1))\n",
        "    \n",
        "    future_layer_1_delta = np.zeros(hidden_dim)\n",
        "    \n",
        "    for position in range(binary_dim):\n",
        "        \n",
        "        X = np.array([[a[position],b[position]]])\n",
        "        layer_1 = layer_1_values[-position-1]\n",
        "        prev_layer_1 = layer_1_values[-position-2]\n",
        "        \n",
        "        # величина ошибки в output layer\n",
        "        layer_2_delta = layer_2_deltas[-position-1]\n",
        "        # величина ошибки в hidden layer\n",
        "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
        "\n",
        "        # обновление всех весов и пробуем заново\n",
        "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
        "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
        "        synapse_0_update += X.T.dot(layer_1_delta)\n",
        "        \n",
        "        future_layer_1_delta = layer_1_delta\n",
        "    \n",
        "\n",
        "    synapse_0 += synapse_0_update * alpha\n",
        "    synapse_1 += synapse_1_update * alpha\n",
        "    synapse_h += synapse_h_update * alpha    \n",
        "\n",
        "    synapse_0_update *= 0\n",
        "    synapse_1_update *= 0\n",
        "    synapse_h_update *= 0\n",
        "    \n",
        "    # вывод на экран процесса обучения\n",
        "    if(j % 1000 == 0):\n",
        "        print(\"Error:\" + str(overallError))\n",
        "        print(\"Pred:\" + str(d))\n",
        "        print(\"True:\" + str(c))\n",
        "        out = 0\n",
        "        for index,x in enumerate(reversed(d)):\n",
        "            out += x*pow(2,index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
        "        print(\"------------\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error:[3.45638663]\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "9 + 60 = 1\n",
            "------------\n",
            "Error:[3.63389116]\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "28 + 35 = 255\n",
            "------------\n",
            "Error:[3.91366595]\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "116 + 44 = 72\n",
            "------------\n",
            "Error:[3.72191702]\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "4 + 73 = 223\n",
            "------------\n",
            "Error:[3.5852713]\n",
            "Pred:[0 0 0 0 1 0 0 0]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "71 + 11 = 8\n",
            "------------\n",
            "Error:[2.53352328]\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "81 + 113 = 162\n",
            "------------\n",
            "Error:[0.57691441]\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "81 + 0 = 81\n",
            "------------\n",
            "Error:[1.42589952]\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "4 + 125 = 129\n",
            "------------\n",
            "Error:[0.47477457]\n",
            "Pred:[0 0 1 1 1 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "39 + 17 = 56\n",
            "------------\n",
            "Error:[0.21595037]\n",
            "Pred:[0 0 0 0 1 1 1 0]\n",
            "True:[0 0 0 0 1 1 1 0]\n",
            "11 + 3 = 14\n",
            "------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dr_g_zyQ1IA",
        "colab_type": "text"
      },
      "source": [
        "Теперь давайте попробуем с помощью Keras построить LSTM нейронную сеть для оценки настроений отзвывов на IMD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlL6I06bQ1IB",
        "colab_type": "text"
      },
      "source": [
        "Данный датасет слишком мал, чтобы преимущества LSTM проявились, однако в учебных целях он подойдет.\n",
        "\n",
        "В тренировке рекуррентных нейронных сетей важную роль играет размер batch, но еще большую роль играет выбор функций loss и optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okh5umFZQ1ID",
        "colab_type": "code",
        "outputId": "a75ac9fa-2d1f-4a50-aa76-abeb0e0d3a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "\n",
        "# обрезание текстов после данного количества слов (среди top max_features наиболее используемые слова)\n",
        "maxlen = 80\n",
        "batch_size = 50 # увеличьте значение для ускорения обучения\n",
        "\n",
        "print('Загрузка данных...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'тренировочные последовательности')\n",
        "print(len(x_test), 'тестовые последовательности')\n",
        "\n",
        "print('Pad последовательности (примеров в x единицу времени)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Построение модели...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# стоит попробовать использовать другие оптимайзер и другие конфигурации оптимайзеров \n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Процесс обучения...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=1, # увеличьте при необходимости\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Результат при тестировании:', score)\n",
        "print('Тестовая точность:', acc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Загрузка данных...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "25000 тренировочные последовательности\n",
            "25000 тестовые последовательности\n",
            "Pad последовательности (примеров в x единицу времени)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n",
            "Построение модели...\n",
            "Процесс обучения...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/1\n",
            "25000/25000 [==============================] - 131s 5ms/step - loss: 0.4575 - accuracy: 0.7865 - val_loss: 0.3800 - val_accuracy: 0.8306\n",
            "25000/25000 [==============================] - 28s 1ms/step\n",
            "Результат при тестировании: 0.3799752759039402\n",
            "Тестовая точность: 0.8306400179862976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mBFA_rPQ1IN",
        "colab_type": "text"
      },
      "source": [
        "Давайте также посмотрим пример в которм будет использоваться другой класс задач - генерация текста на основе тренировочного текста. В задачу нейросети будет входить обучившись на тексте Алиса в стране чудес и начать генерировать текст похожий на тот, что можно встретить в этой книге. Также в этом примере будет использоваться GRU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC2lYh6d2WXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu75EojPQ1IP",
        "colab_type": "code",
        "outputId": "641d3699-332c-4229-b82a-e4ffbda03880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "import numpy as np\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
        "from keras.models import Sequential\n",
        "\n",
        "\n",
        "# построчное чтение из примера с текстом \n",
        "with open(\"/content/drive/My Drive/Введение в нейронные сети/5 Урок/alice_in_wonderland.txt\", 'rb') as _in:\n",
        "    lines = []\n",
        "    for line in _in:\n",
        "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "        lines.append(line)\n",
        "text = \" \".join(lines)\n",
        "chars = set([c for c in text])\n",
        "nb_chars = len(chars)\n",
        "\n",
        "\n",
        "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
        "# ID and a specific character. The numerical ID will correspond to a column\n",
        "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
        "# число при использовании one-hot кодировки для представление входов символов\n",
        "char2index = {c: i for i, c in enumerate(chars)}\n",
        "index2char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "# для удобства выберете фиксированную длину последовательность 10 символов \n",
        "SEQLEN, STEP = 10, 1\n",
        "input_chars, label_chars = [], []\n",
        "\n",
        "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
        "for i in range(0, len(text) - SEQLEN, STEP):\n",
        "    input_chars.append(text[i: i + SEQLEN])\n",
        "    label_chars.append(text[i + SEQLEN])\n",
        "\n",
        "\n",
        "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
        "\n",
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
        "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "    for j, ch in enumerate(input_char):\n",
        "        X[i, j, char2index[ch]] = 1\n",
        "    y[i, char2index[label_chars[i]]] = 1\n",
        "\n",
        "\n",
        "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
        "BATCH_SIZE, HIDDEN_SIZE = 64, 256\n",
        "NUM_ITERATIONS = 2 # 25 должно быть достаточно\n",
        "NUM_EPOCHS_PER_ITERATION = 6\n",
        "NUM_PREDS_PER_EPOCH = 50\n",
        "\n",
        "# Create a super simple recurrent neural network. There is one recurrent\n",
        "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
        "# encoded input layer. This is followed by a Dense fully-connected layer\n",
        "# across the set of possible next characters, which is converted to a\n",
        "# probability score via a standard softmax activation with a multi-class\n",
        "# cross-entropy loss function linking the prediction to the one-hot\n",
        "# encoding character label.\n",
        "\n",
        "'''\n",
        "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный \n",
        "слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию \n",
        "с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
        "'''\n",
        "\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    LSTM(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
        "        HIDDEN_SIZE,\n",
        "        return_sequences=False,\n",
        "        input_shape=(SEQLEN, nb_chars),\n",
        "        unroll=True\n",
        "    )\n",
        ")\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "\n",
        "# выполнение серий тренировочных и демонстрационных итераций \n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "\n",
        "    # для каждой итерации запуск передачи данных в модель \n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Итерация #: {iteration + 1}/{NUM_ITERATIONS}\")\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "\n",
        "    # Select a random example input sequence.\n",
        "    test_idx = np.random.randint(len(input_chars))\n",
        "    test_chars = input_chars[test_idx]\n",
        "\n",
        "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
        "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
        "    print(\"Генерация из посева: %s\" % (test_chars))\n",
        "    print(test_chars, end=\"\")\n",
        "    for i in range(NUM_PREDS_PER_EPOCH):\n",
        "\n",
        "        # здесь one-hot encoding.\n",
        "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
        "        for j, ch in enumerate(test_chars):\n",
        "            X_test[0, j, char2index[ch]] = 1\n",
        "\n",
        "        # осуществление предсказания с помощью текущей модели.\n",
        "        pred = model.predict(X_test, verbose=0)[0]\n",
        "        y_pred = index2char[np.argmax(pred)]\n",
        "\n",
        "        # вывод предсказания добавленного к тестовому примеру \n",
        "        print(y_pred, end=\"\")\n",
        "\n",
        "        # инкрементация тестового примера содержащего предсказание\n",
        "        test_chars = test_chars[1:] + y_pred\n",
        "print()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Итерация #: 1/2\n",
            "Epoch 1/6\n",
            "158773/158773 [==============================] - 140s 880us/step - loss: 2.2157\n",
            "Epoch 2/6\n",
            "158773/158773 [==============================] - 135s 852us/step - loss: 1.7484\n",
            "Epoch 3/6\n",
            "158773/158773 [==============================] - 136s 856us/step - loss: 1.5406\n",
            "Epoch 4/6\n",
            "158773/158773 [==============================] - 136s 856us/step - loss: 1.4001\n",
            "Epoch 5/6\n",
            "158773/158773 [==============================] - 143s 902us/step - loss: 1.2981\n",
            "Epoch 6/6\n",
            "158773/158773 [==============================] - 135s 849us/step - loss: 1.2143\n",
            "Генерация из посева: ou usually\n",
            "ou usually the project gutenberg-tm license the mouse was on==================================================\n",
            "Итерация #: 2/2\n",
            "Epoch 1/6\n",
            "158773/158773 [==============================] - 136s 855us/step - loss: 1.1421\n",
            "Epoch 2/6\n",
            "158773/158773 [==============================] - 135s 851us/step - loss: 1.0752\n",
            "Epoch 3/6\n",
            "158773/158773 [==============================] - 136s 857us/step - loss: 1.0129\n",
            "Epoch 4/6\n",
            "158773/158773 [==============================] - 139s 876us/step - loss: 0.9539\n",
            "Epoch 5/6\n",
            "158773/158773 [==============================] - 135s 850us/step - loss: 0.8990\n",
            "Epoch 6/6\n",
            "158773/158773 [==============================] - 136s 854us/step - loss: 0.8459\n",
            "Генерация из посева: n contempt\n",
            "n contemptuous tones and the mouse was sitting on the treach\n",
            "CPU times: user 43min 25s, sys: 2min 19s, total: 45min 44s\n",
            "Wall time: 27min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX6_MRk99_aC",
        "colab_type": "text"
      },
      "source": [
        "#Results:\n",
        "\n",
        "- Базовые параметры:\n",
        "  -  loss: 2.3124\n",
        "\n",
        "    court with the said the said the said the said the said the said the said the said the said the said the said \n",
        "\n",
        "- batch = 64, epochs = 3:\n",
        "  - loss: 1.6733\n",
        "\n",
        "    Генерация из посева:  and glove\n",
        "\n",
        "    and gloved and she was she was she was she was she was she was she was she was she was she was she was she wa\n",
        "\n",
        "- NUM_EPOCHS_PER_ITERATION = 5 (при увеличении этого параметра заикание осталось, но куски повторяемого текста увеличились с двух слов до нескольких)\n",
        "  - loss: 1.4450\n",
        "\n",
        "    Генерация из посева: d inclined\n",
        "\n",
        "    d inclined the caterpillar and what it was the seamed to herself, and whither was the caterpillar and what it was the seamed to herself, and whither was the caterpillar and what it was the seamed to herself, and whither was the ...\n",
        "\n",
        "- BATCH_SIZE, HIDDEN_SIZE = 64, 256\n",
        "\n",
        "  NUM_ITERATIONS = 2\n",
        "\n",
        "  NUM_EPOCHS_PER_ITERATION = 6\n",
        "\n",
        "  NUM_PREDS_PER_EPOCH = 50\n",
        "\n",
        "  - loss: 1.2143\n",
        "\n",
        "    Генерация из посева: ou usually\n",
        "\n",
        "    ou usually the project gutenberg-tm license the mouse was on==================================================\n",
        "\n",
        "  - loss: 0.8459\n",
        "\n",
        "    Генерация из посева: n contempt\n",
        "\n",
        "    n contemptuous tones and the mouse was sitting on the treach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si7CA-ZpzPKc",
        "colab_type": "text"
      },
      "source": [
        "#Практическое задание\n",
        "1. Попробуйте изменить параметры нейронной сети работающей с датасетом imdb так, чтобы улучшить ее точность. Приложите анализ.\n",
        "\n",
        "2. Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения.\n",
        "\n",
        "3. ** Попробуйте на numpy реализовать нейронную сеть архитектуры LSTM\n",
        "  - это задание я пропущу, т.к. понимаю как это работает только поверхностно.\n",
        "\n",
        "4. ** Предложите свои варианты решения проблемы исчезающего градиента в RNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhxB4Zgy9fk7",
        "colab_type": "text"
      },
      "source": [
        "#Вывод\n",
        "1. \n",
        "\n",
        "2. Не знаю на сколько ответы НС \"осознанны\" и верны, но последние два результата выглядят наиболее связными и правдоподобными из всех перепробованных вариантов. При меньшем ко-ве эпох и большем кол-ве итераций развивался синдром турета и НС начинала заикаться и постоянно дублировать пары слов.\n",
        "\n",
        "3. \n",
        "\n",
        "4. ** Предложите свои варианты решения проблемы исчезающего градиента в RNN\n",
        "\n",
        "  Варианты не мои, а общеиспользуемые.\n",
        "  - функция активации leaky relu\n",
        "  - функция активации swish\n",
        "  - добавление градиентного шума "
      ]
    }
  ]
}